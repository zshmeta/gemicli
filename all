


// src/config/default.json







// src/main.js
import ora from 'ora';
import fs from 'fs';
import path from 'path';
import { listChat, setChat, initAI } from './commands/index';
import chalk from 'chalk';
import cfonts from 'cfonts';

const configPath = path.join(__dirname, 'config/default.json');

async function gemicli() {
  // Styles
 
  const commands = {
    '--chat': async (args) => {
      const spinner = ora('Processing...').start();
      try {
        await chat(args[0]);
        spinner.succeed('Done');
      } catch (error) {
        spinner.fail('Error');
        console.error(chalk.red(error.message));
      } finally {
        process.exit();
      }
    },
    '--image-embed': async (args) => {
      const spinner = ora('Processing...').start();
      try {
        await imageEmbed(args[0]);
        spinner.succeed('Done');
      } catch (error) {
        spinner.fail('Error');
        console.error(chalk.red(error.message));
      } finally {
        process.exit();
      }
    },
    '--set-model': (args) => {
      const config = JSON.parse(fs.readFileSync(configPath));
      config.model = args[0];
      fs.writeFileSync(configPath, JSON.stringify(config, null, 2));
      console.log(chalk.green(`Model set to: ${args[0]}`));
      process.exit();
    },
    '--set-key': (args) => {
      const config = JSON.parse(fs.readFileSync(configPath));
      config.apiKey = args[0];
      fs.writeFileSync(configPath, JSON.stringify(config, null, 2));
      console.log(chalk.green('API key set successfully'));
      process.exit();
    },
    '--history-ls': async() => {
      await listChat();
      process.exit();
    },
    '--history-set': async (args) => {
      await setChat(args[0]);
      process.exit();
    },
    '--init': async () => {
      await initAI();
    }
  };

  const args = process.argv.slice(2);
  const command = args[0];

  if (args.length === 0 || command === '--help' || command === '-h') {
    // Display help menu
    cfonts.say('Gemicli', {
      font: '3d',              
      align: 'center',              
      gradient: ['blue', 'green', 'magenta', 'yellow'],         
      background: 'transparent',         
      space: true,                
      maxLength: '0',             
      transitionGradient: true,  
      env: 'node'                 
    });
    console.log(chalk.cyanBright('--chat <message>                 Chat with the AI using the provided message.'));
    console.log(chalk.cyanBright('--image-embed <image_path>       Embed an image using the specified path.'));
    console.log(chalk.cyanBright('--set-model <model_name>         Set the AI model to use.'));
    console.log(chalk.cyanBright('--set-key <api_key>              Set the API key for authentication.'));
    console.log(chalk.cyanBright('--history-ls                     List the chat history.'));
    console.log(chalk.cyanBright('--history-set <history>          Set the chat history.'));
    console.log(chalk.cyanBright('--init                           Initialize the AI.'));
    process.exit();
  } else if (commands[command]) {
    commands[command](args.slice(1));
  } else {
    console.log(chalk.red(`Unknown command: ${command}`));
    process.exit(1);
  }
}

export  { gemicli };







// src/commands/imageEmbed.js
import { GoogleGenerativeAI } from '@google/generative-ai';
import { fs } from 'fs';
import { genAi, model } from '../utils/config'

// Converts local file information to a GoogleGenerativeAI.Part object.
function fileToGenerativePart(path, mimeType) {
  return {
    inlineData: {
      data: Buffer.from(fs.readFileSync(path)).toString("base64"),
      mimeType
    },
  };
}

async function genAiMulti(prompt, imageParts) {
  // The Gemini 1.5 models are versatile and work with both text-only and multimodal prompts
  const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

  const prompt = {
    text: prompt,
  };

  const imageParts = [
     imageParts
  ];

  const result = await model.generateContent([prompt, ...imageParts]);
  const response = await result.response;
  const text = response.text();
  console.log(text);
}

export { genAiMulti }






// src/commands/genAI.js
import { GoogleGenerativeAI } from '@google/generative-ai';
import { genAI, model } from '../utils/config';


async function genAi(prompt) {
  // The Gemini 1.5 models are versatile and work with multi-turn conversations (like chat)
  
  const genModels = genAI.getGenerativeModel(model)

  const chat = genModels.startChat({
    history: [
      {
        role: "user",
        parts: [{ text: "I am on my terminal right now and hope you will help me for some stuff" }],
      },
      {
        role: "model",
        parts: [{ text: "Ok i am ready. are you ready?" }],
      },
    ],
    generationConfig: {
      maxOutputTokens: 300,
    },
  });

  const msg = {
    role: "user",
    parts: [{ text: prompt }],
  };

  //...

const result = await genModels.generateContentStream([prompt, ...imageParts]);

let text = '';
for await (const chunk of result.stream) {
  const chunkText = chunk.text();
  console.log(chunkText);
  text += chunkText;
}

}

export { genAi };






// src/commands/initAI.js
#!/usr/bin/env node

import fs from 'fs';
import path from 'path';
import chalk from 'chalk';
import { fileURLToPath } from 'url';
import readline from 'readline';
import { fetchModels } from '../utils/api'; // Ensure this is correctly implemented and imported

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const envPath = path.resolve(__dirname, '../config/.env');
// if file exists move it to .env.bak
if (fs.existsSync(envPath)) {
    fs.renameSync(envPath, `${envPath}.bak`);
}

// Function to write a key-value pair to the .env file
function writeEnv(key, value) {
    const envContent = `${key}=${value}\n`;
    fs.appendFileSync(envPath, envContent, { flag: 'a' });
}

// Create readline interface
const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout
});

async function askQuestion(query) {
    return new Promise(resolve => rl.question(query, resolve));
}

async function initAI() {
    try {
        console.log(chalk.blue('Initializing gemicli configs...'));

        // Ask the user for their AI choice
        const aiChoices = ['gemini'];
        const geminiModels = ['gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-pro-vision'];

        console.log('Which AI to use?');
        aiChoices.forEach((choice, index) => console.log(`${index + 1}. ${choice}`));
        const aiChoiceIndex = await askQuestion('Enter number of your choice: ');
        const ai = aiChoices[parseInt(aiChoiceIndex) - 1];
        if (!ai) {
            console.error(chalk.red('Invalid choice'));
            process.exit(1);
        }
        writeEnv('AI', ai);

        // Ask the user for their API key
        const apiKey = await askQuestion('Enter your API key: ');
        writeEnv('GOOGLE_API_KEY', apiKey);

        // Ask the user which model to use
        console.log('Which model to use?');
        geminiModels.forEach((choice, index) => console.log(`${index + 1}. ${choice}`));
        const modelChoiceIndex = await askQuestion('Enter number of your choice: ');
        const model = geminiModels[parseInt(modelChoiceIndex) - 1];
        if (!model) {
            console.error(chalk.red('Invalid choice'));
            process.exit(1);
        }

        writeEnv('MODEL', model);

        // Ask for the system prompt
        const systemPrompt = await askQuestion('Enter your system prompt (default: "You are a helpful assistant. You will do your best to answer any queries with the appropriate format."): ') || 'You are a helpful assistant. You will do your best to answer any queries with the appropriate format.';
        writeEnv('SYSTEM_PROMPT', `"${systemPrompt}"`);

        console.log(chalk.green('gemicli configs initialized successfully!'));
    } catch (error) {
        console.error(chalk.red('Error initializing gemicli configs:'), error);
    } finally {
        rl.close();
    }
}

export { initAI }







// src/commands/setChat.js
import chalk from 'chalk';
import { fetchChats } from '../utils/api';

async function setChat(chatId) {
	// Implement logic to fetch the chat from the API

	try {
		const chat = await fetchChats(chatId);
		console.log(chalk.green(`Selected chat with ID: ${chat.id}`));
		console.log(chalk.blue(`Chat content: ${chat.content}`));
		// Implement logic to set the chat as the current context if necessary
	} catch (error) {
		console.error(chalk.red('Error fetching chat:', error.message));
	}
}

export {setChat};







// src/commands/listChat.js
import chalk from 'chalk';
import { fetchChats } from '../utils/api';

async function listChat() {
	try {
		const chats = await fetchChats();
		if (chats.length === 0) {
			console.log(chalk.yellow('No chat history found.'));
			return;
		}
		chats.forEach((chat, index) => {
			console.log(`${index + 1}. ${chat.id} - ${chat.summary}`);
		});
	} catch (error) {
		console.error(chalk.red('Error fetching chat history:', error.message));
	}
}

export {listChat};







// src/commands/index.js
import {setChat} from './setChat.js';
import {listChat} from './listChat.js';
import {initAI} from './initAI.js';
import {genAi} from './genAi.js';

export {
    listChat, 
    setChat, 
    initAI,
    genAi
};







// src/utils/config.js
import { GoogleGenerativeAI } from "@google/generative-ai";
import path from "path";
import fs from "fs";

// Acccess api key from the env file

const envPath = path.join(__dirname, '../config/.env');
if (fs.existsSync(envPath)) {
	const envContent = fs.readFileSync(envPath);
	const envVars = envContent.toString().split('\n');
	envVars.forEach(varLine => {
		const [key, value] = varLine.split('=');
		process.env[key] = value;
	});
}
const genAI = new GoogleGenerativeAI({
	apiKey: process.env.GOOGLE_API_KEY,
});

const model = () => ({
	model: process.env.GOOGLE_MODEL,
});


export { model, genAI }






// src/utils/logger.js







// src/utils/api.js
import axios from 'axios';
import fs from 'fs';
import path from 'path';

// Load API key from environment file
const envPath = path.join(__dirname, '../config/.env');
if (fs.existsSync(envPath)) {
	const envContent = fs.readFileSync(envPath);
	const envVars = envContent.toString().split('\n');
	envVars.forEach(varLine => {
		const [key, value] = varLine.split('=');
		process.env[key] = value;
	});
}

const apiClient = axios.create({
	baseURL: 'https://generativelanguage.googleapis.com/v1beta',
	headers: {
		'Authorization': `Bearer ${process.env.API_KEY}`,
	},
});

async function fetchModels() {
	try {
		const response = await apiClient.get('/models');
		return response.data.models.map(model => model.name);
	} catch (error) {
		console.error('Error fetching models:', error);
		throw error;
	}
}

async function fetchChats() {
	try {
		const response = await apiClient.get('/chats');
		return response.data.chats;
	} catch (error) {
		console.error('Error fetching previous chats:', error);
		throw error;
	}
}

export { fetchModels, fetchChats }



